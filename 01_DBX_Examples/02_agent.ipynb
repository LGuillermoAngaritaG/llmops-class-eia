{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1d353afa-013c-435c-82ef-c5aade9a0c58",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%pip install --upgrade langchain-databricks langchain-community langchain databricks-sql-connector nltk rouge-score langchain-experimental duckduckgo-search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "30914844-caaa-41ac-9e0d-2e2fac9348be",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%restart_python or dbutils.library.restartPython()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "fe6ff43a-1ab1-47d6-98b3-a5bde13322fb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Proof of concept (POC)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4586de27-31af-4213-b099-f2297d71dc68",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Here we want to make a simple chain that takes a question and gives an answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2e9da268-2c02-44a5-9b4a-6f761bf4cbf4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from langchain import hub\n",
    "from langchain.agents import Tool, create_react_agent\n",
    "from langchain.agents.agent import AgentExecutor\n",
    "from langchain_experimental.utilities import PythonREPL\n",
    "from langchain_community.tools import DuckDuckGoSearchRun\n",
    "from langchain.tools import tool\n",
    "import mlflow\n",
    "from langchain_databricks import ChatDatabricks\n",
    "\n",
    "chat_model = ChatDatabricks(\n",
    "                            endpoint=\"databricks-meta-llama-3-1-70b-instruct\",\n",
    "                            temperature=0.1,\n",
    "                            max_tokens=250,\n",
    "                        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5acf0e8f-7be6-439f-89d8-a0b044d11edd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Tools\n",
    "python_repl = PythonREPL()\n",
    "search = DuckDuckGoSearchRun()\n",
    "repl_tool = Tool(\n",
    "    name=\"python_repl\",\n",
    "    description=\"A Python shell. Use this to execute python commands. Input should be a valid python command. If you want to see the output of a value, you should print it out with `print(...)`.\",\n",
    "    func=python_repl.run,\n",
    ")\n",
    "search_tool = Tool(\n",
    "    name=\"search\",\n",
    "    description=\"Search the web for information\",\n",
    "    func=search.run,\n",
    ")\n",
    "@tool\n",
    "def make_a_poem(topic: str) -> str:\n",
    "    \"\"\"Make a poem about a topic.\"\"\"\n",
    "    prompt = f\"Make a poem about {topic}\"\n",
    "    response = chat_model.invoke(prompt)\n",
    "    return response.content\n",
    "\n",
    "# prompt\n",
    "prompt = hub.pull(\"hwchase17/react\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8a4035ea-2404-4797-bd4a-99a936025166",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# create agents\n",
    "agent = create_react_agent(\n",
    "    llm=chat_model,\n",
    "    tools=[repl_tool, search_tool, make_a_poem],\n",
    "    prompt=prompt,\n",
    ")\n",
    "agent_executor = AgentExecutor(\n",
    "    agent=agent,\n",
    "    tools=[repl_tool, search_tool, make_a_poem],\n",
    "    max_iterations=5,\n",
    "    handle_parsing_errors=True,\n",
    "    verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "33551476-fa6c-4c8b-bb54-574c3e22cb19",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "query =\"what is EPM in colombia?\"\n",
    "agent_executor.invoke({\"input\": query})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7287e4ad-e9c3-4e2b-8b44-bee11b411b8a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# ML Flow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b09119c4-a2d7-4e72-a9f9-35bbaa3ae376",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6b12fba2-f31c-4b44-b1cf-73c0bc967c1c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def log_chain_to_mlflow(chain, wrapper, experiment_name: str = \"example-chain\"):\n",
    "    \"\"\"Helper function for logging a chain to MLflow\"\"\"\n",
    "    # Get Path for MLflow\n",
    "    if IS_DATABRICKS:\n",
    "        # Use workspace path for Databricks\n",
    "        experiment_path = f\"/Shared/{experiment_name}\"\n",
    "        try:\n",
    "            experiment = mlflow.get_experiment_by_name(experiment_path)\n",
    "            if experiment is None:\n",
    "                mlflow.create_experiment(experiment_path)\n",
    "            mlflow.set_experiment(experiment_path)\n",
    "        except Exception as e:\n",
    "            print(f\"Error setting up Databricks experiment: {e}\")\n",
    "            raise\n",
    "    else:\n",
    "        # Local experiment setup\n",
    "        experiment = mlflow.get_experiment_by_name(experiment_name)\n",
    "        if experiment is None:\n",
    "            mlflow.create_experiment(experiment_name)\n",
    "        mlflow.set_experiment(experiment_name)\n",
    "\n",
    "    # Define model signature\n",
    "    from mlflow.models.signature import ModelSignature\n",
    "    from mlflow.types import Schema, ColSpec, DataType\n",
    "    \n",
    "    # Define input schema (question column)\n",
    "    input_schema = Schema([\n",
    "        ColSpec(DataType.string, \"input\")\n",
    "    ])\n",
    "    \n",
    "    # Define output schema (model returns string)\n",
    "    output_schema = Schema([\n",
    "        ColSpec(DataType.string)\n",
    "    ])\n",
    "    \n",
    "    # Create signature\n",
    "    signature = ModelSignature(inputs=input_schema, outputs=output_schema)\n",
    "\n",
    "    # Start logging of model\n",
    "    with mlflow.start_run() as run:\n",
    "        config = chain.get_config()\n",
    "        mlflow.log_params({\n",
    "            \"model_name\": config[\"model_name\"],\n",
    "            \"temperature\": config[\"temperature\"],\n",
    "            \"environment\": \"databricks\" if IS_DATABRICKS else \"local\"\n",
    "        })\n",
    "\n",
    "        # Create temporary file for prompt template\n",
    "        prompt_path = None\n",
    "        with tempfile.NamedTemporaryFile(mode='w', delete=False, suffix='.txt') as f:\n",
    "            f.write(str(config[\"prompt_template\"]))\n",
    "            prompt_path = f.name\n",
    "\n",
    "        try:\n",
    "            # Create wrapper\n",
    "            wrapper = wrapper(chain)\n",
    "            # Log the model with requirements and signature\n",
    "            requirements = [\n",
    "            \"langchain-databricks\",\n",
    "            \"langchain-community\",\n",
    "            \"langchain databricks-sql-connector\",\n",
    "            \"langchain-experimental\",\n",
    "            \"duckduckgo-search\"\n",
    "            ]\n",
    "            if IS_DATABRICKS:\n",
    "                requirements.append(\"databricks-mlflow\")\n",
    "            \n",
    "            logged_model = mlflow.pyfunc.log_model(\n",
    "                artifact_path=\"artifacts\",\n",
    "                python_model=wrapper,\n",
    "                artifacts={\"prompt_template\": prompt_path},\n",
    "                pip_requirements=requirements,\n",
    "                signature=signature  # Add the signature here\n",
    "            )\n",
    "        finally:\n",
    "            if prompt_path and os.path.exists(prompt_path):\n",
    "                os.unlink(prompt_path)\n",
    "\n",
    "    return run.info.run_id, logged_model.model_uri\n",
    "\n",
    "def load_chain_from_mlflow(run_id: str, experiment_name = \"example_chain\", ):\n",
    "    \"\"\"\n",
    "    Helper function to load a chain from MLFlow\n",
    "    \"\"\"\n",
    "    try:\n",
    "        if IS_DATABRICKS:\n",
    "            model_uri = f\"runs:/{run_id}/artifacts\"\n",
    "            chain = mlflow.pyfunc.load_model(model_uri)\n",
    "            return chain\n",
    "        else:\n",
    "            model_uri = f\"runs:/{run_id}/{experiment_name}\"\n",
    "            chain = mlflow.pyfunc.load_model(model_uri)\n",
    "            return chain\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading chain from MLflow: {e}\")\n",
    "        raise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "67007d81-fc5c-4b02-ba5c-76bb316791c4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Defining chain as a Model with a wrapper"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d675c218-eee6-425f-a916-71a65166004b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Class for the chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "966249a5-e6e0-42a5-8681-71a832bd9892",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import mlflow\n",
    "from langchain_databricks import ChatDatabricks\n",
    "import pandas as pd\n",
    "from typing import Dict, Any\n",
    "import os\n",
    "import tempfile\n",
    "\n",
    "try:\n",
    "    import databricks.mlflow\n",
    "    IS_DATABRICKS = True\n",
    "    import dbutils\n",
    "except ImportError:\n",
    "    IS_DATABRICKS = False\n",
    "\n",
    "class myChain:\n",
    "    def __init__(self, model_name, prompt_template = hub.pull(\"hwchase17/react\"), temperature=0):\n",
    "        self.model_name = model_name\n",
    "        self.prompt_template = prompt_template\n",
    "        self.temperature = temperature\n",
    "\n",
    "    def run_chain(self, query):\n",
    "        # Model\n",
    "        chat_model = ChatDatabricks(\n",
    "                            endpoint=self.model_name,\n",
    "                            temperature=0.1,\n",
    "                            max_tokens=250,\n",
    "                        )\n",
    "        # Tools\n",
    "        python_repl = PythonREPL()\n",
    "        search = DuckDuckGoSearchRun()\n",
    "        repl_tool = Tool(\n",
    "            name=\"python_repl\",\n",
    "            description=\"A Python shell. Use this to execute python commands. Input should be a valid python command. If you want to see the output of a value, you should print it out with `print(...)`.\",\n",
    "            func=python_repl.run,\n",
    "        )\n",
    "        search_tool = Tool(\n",
    "            name=\"search\",\n",
    "            description=\"Search the web for information\",\n",
    "            func=search.run,\n",
    "        )\n",
    "        @tool\n",
    "        def make_a_poem(topic: str) -> str:\n",
    "            \"\"\"Make a poem about a topic.\"\"\"\n",
    "            prompt = f\"Make a poem about {topic}\"\n",
    "            response = chat_model.invoke(prompt)\n",
    "            return response.content\n",
    "        \n",
    "        # Agents\n",
    "        agent = create_react_agent(\n",
    "            llm=chat_model,\n",
    "            tools=[repl_tool, search_tool, make_a_poem],\n",
    "            prompt=self.prompt_template,\n",
    "        )\n",
    "        agent_executor = AgentExecutor(\n",
    "            agent=agent,\n",
    "            tools=[repl_tool, search_tool, make_a_poem],\n",
    "            max_iterations=5,\n",
    "            handle_parsing_errors=True,\n",
    "            verbose=True\n",
    "        )\n",
    "        return agent_executor.invoke({\"input\": query})['output']\n",
    "\n",
    "    def __call__(self, query):\n",
    "        result = self.run_chain(query)\n",
    "        return result\n",
    "    \n",
    "    def get_config(self) -> Dict[str, Any]:\n",
    "        \"\"\"Get chain configuration for MLflow tracking\"\"\"\n",
    "        return {\n",
    "            \"model_name\": self.model_name,\n",
    "            \"temperature\": self.temperature,\n",
    "            \"prompt_template\": self.prompt_template\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "42494b4a-9642-4138-8f99-0d6847653528",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Wrapper for MLFlow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dda70f8b-64f0-44d5-8eff-8444fe0edad0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "class myChainWrapper(mlflow.pyfunc.PythonModel):\n",
    "    def __init__(self, chain=None):\n",
    "        self.chain = chain\n",
    "        \n",
    "    def predict(self, context, model_input):\n",
    "        \"\"\"\n",
    "        :param context: MLflow model context\n",
    "        :param model_input: DataFrame or Series containing YouTube URLs\n",
    "        :return: List of summaries\n",
    "        \"\"\"\n",
    "        questions = model_input['input'].to_list()\n",
    "            \n",
    "        return [self.chain(question) for question in questions]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3f131673-b25c-4f49-a139-3eb596b8b047",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# create an experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "58bdf61d-3e7b-4746-bbb6-dc9bb7d6bf4f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "model_name = \"databricks-meta-llama-3-1-70b-instruct\"\n",
    "\n",
    "chain = myChain(model_name=model_name)\n",
    "\n",
    "run_id, model_uri = log_chain_to_mlflow(chain=chain, \n",
    "                             wrapper=myChainWrapper, experiment_name=\"/Users/guillermo.angarita.gutierrez@gmail.com/agent_chain\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f71a43b1-ef3b-4cae-8dea-95ce4ee2e3a5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "run_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "485909ff-8e7c-4467-8791-9b1053eb109d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "model_uri"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "171320b8-fe15-49c8-8446-f1ac7e8799f9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Monitoring model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a3dcc76a-7612-476b-b00b-c9517dcef781",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f512e761-8858-405c-be08-a1bea1d3d869",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import psutil\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "import time\n",
    "mlflow.enable_system_metrics_logging()\n",
    "\n",
    "class PerformanceMonitor:\n",
    "    def __init__(self):\n",
    "        self.metrics_history = []\n",
    "    \n",
    "    def measure_latency(self, func, *args, **kwargs):\n",
    "        \"\"\"\n",
    "        Measure execution time of a function\n",
    "        \n",
    "        :param func: Function to measure\n",
    "        :param args: Positional arguments for the function\n",
    "        :param kwargs: Keyword arguments for the function\n",
    "        :return: tuple of (results, execution_time)\n",
    "        \"\"\"\n",
    "        start_time = time.time()\n",
    "        results = func(*args, **kwargs)  # Just store the results directly\n",
    "        end_time = time.time()\n",
    "        return results, end_time - start_time\n",
    "    \n",
    "    def measure_resource_usage(self):\n",
    "        \"\"\"Measure CPU and memory usage\"\"\"\n",
    "        cpu_percent = psutil.cpu_percent()\n",
    "        memory_info = psutil.Process().memory_info()\n",
    "        return {\n",
    "            'cpu_percent': cpu_percent,\n",
    "            'memory_mb': memory_info.rss / 1024 / 1024\n",
    "        }\n",
    "    \n",
    "    def calculate_text_metrics(self, question: str, response: str) -> dict:\n",
    "        \"\"\"\n",
    "        Calculate text-based metrics like reduction percentage and lengths\n",
    "        \"\"\"\n",
    "        question_length = len(question.split())\n",
    "        response_length = len(response.split())\n",
    "        \n",
    "        return {\n",
    "            'question_length': question_length,\n",
    "            'response_length': response_length,\n",
    "        }\n",
    "    \n",
    "    def log_performance(self, latency, question: str, response:str, resource_usage: dict):\n",
    "        \"\"\"\n",
    "        Log all performance metrics including text metrics\n",
    "        \"\"\"\n",
    "        text_metrics = self.calculate_text_metrics(question, response)\n",
    "        metrics = {\n",
    "            'latency': latency,\n",
    "            **resource_usage,\n",
    "            **text_metrics\n",
    "        }\n",
    "        self.metrics_history.append(metrics)\n",
    "        self.log_metrics(metrics)\n",
    "        return metrics\n",
    "    \n",
    "    def log_metrics(self, metrics_dict):\n",
    "        \"\"\"Log metrics to MLflow\"\"\"\n",
    "        with mlflow.start_run():\n",
    "            mlflow.log_metrics(metrics_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1f948328-ec90-44f6-b9af-ddd66f36f82c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def plot_metrics_over_time(metrics_history):\n",
    "    \"\"\"\n",
    "    Create interactive plots for metrics over time including text metrics\n",
    "    \n",
    "    :param metrics_history: List of dictionaries containing metrics data\n",
    "    \"\"\"\n",
    "    df = pd.DataFrame(metrics_history)\n",
    "    \n",
    "    # Latency plot\n",
    "    fig_latency = px.line(df, y='latency', title='Inference Latency Over Time')\n",
    "    fig_latency.show()\n",
    "    \n",
    "    # Resource usage plot\n",
    "    fig_resources = go.Figure()\n",
    "    fig_resources.add_trace(go.Scatter(y=df['cpu_percent'], name='CPU %'))\n",
    "    fig_resources.add_trace(go.Scatter(y=df['memory_mb'], name='Memory (MB)'))\n",
    "    fig_resources.update_layout(title='Resource Usage Over Time')\n",
    "    fig_resources.show()\n",
    "    \n",
    "    # Text metrics plot\n",
    "    fig_text = go.Figure()\n",
    "    text_metrics = ['question_length', 'response_length']\n",
    "    for metric in text_metrics:\n",
    "        if metric in df.columns:\n",
    "            fig_text.add_trace(go.Scatter(y=df[metric], name=metric))\n",
    "    fig_text.update_layout(title='Text Metrics Over Time')\n",
    "    fig_text.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c6e50072-40cf-454b-b93e-fd8e29afa5ac",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Inference model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c24a6ccc-6667-4dce-8844-bce3e66828e0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def process_queries(input_df, ml_flow_chain, monitor):\n",
    "    \"\"\"\n",
    "    Process multiple videos with monitoring\n",
    "    \n",
    "    :param video_ids: List of YouTube video IDs or URLs\n",
    "    :return: tuple of (list of summaries, list of metrics)\n",
    "    \"\"\"\n",
    "    # Process all videos\n",
    "    responses = []\n",
    "    metrics_list = []\n",
    "    \n",
    "    # Generate summaries with latency measurement\n",
    "    results, total_latency = monitor.measure_latency(\n",
    "        lambda: ml_flow_chain.predict(input_df)\n",
    "    )\n",
    "    \n",
    "    # Calculate average latency per video\n",
    "    avg_latency = total_latency / len(input_df)\n",
    "    \n",
    "    # Process each result\n",
    "    for idx, result in enumerate(results):\n",
    "        # Measure resource usage\n",
    "        resource_usage = monitor.measure_resource_usage()\n",
    "\n",
    "        # Log metrics for each video\n",
    "        metrics = monitor.log_performance(avg_latency, input_df['input'].iloc[idx], result, resource_usage)\n",
    "        \n",
    "        responses.append(result)\n",
    "        metrics_list.append(metrics)\n",
    "    \n",
    "    return responses, metrics_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c1a30189-5ef5-4870-9ac3-68fce986c6d8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Initialize the performance monitor\n",
    "monitor = PerformanceMonitor()\n",
    "\n",
    "ml_flow_chain = mlflow.pyfunc.load_model(model_uri)\n",
    "\n",
    "questions = [\"Qué es EPM en Colombia?\",\n",
    "              \"What is Databricks?\",\n",
    "              \"Que es langchain?\"]\n",
    "\n",
    "import pandas as pd\n",
    "input_df = pd.DataFrame({'input': questions})\n",
    "\n",
    "answers, metrics = process_queries(input_df, ml_flow_chain, monitor)\n",
    "plot_metrics_over_time(monitor.metrics_history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "441673d9-6c33-4160-a4c6-13d93a3ccbfc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Evaluate Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1ac5b4ee-83ef-4c06-8ac0-dc0a1476fda8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "12657de5-e284-4f04-8888-5c1b4afab901",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "from rouge_score import rouge_scorer\n",
    "import nltk\n",
    "from typing import List\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "# Download required NLTK data\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dd4128d2-91dc-4c04-92df-3b6278ad8e38",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def evaluate_model_with_mlflow(model, test_data: List[Dict[str, str]], experiment_name: str):\n",
    "    \"\"\"Evaluate model and log results to MLflow\"\"\"\n",
    "    mlflow.set_experiment(experiment_name)\n",
    "    \n",
    "    with mlflow.start_run(run_name=\"model_evaluation\") as run:\n",
    "        all_metrics = []\n",
    "        \n",
    "        # Evaluate each test example\n",
    "        for i, example in enumerate(test_data):\n",
    "            try:\n",
    "                # Create DataFrame input\n",
    "                input_df = pd.DataFrame({'input': [example['question']]})\n",
    "                \n",
    "                # Generate prediction\n",
    "                result = model.predict(input_df)[0]  # Get first result\n",
    "                \n",
    "                # Calculate metrics\n",
    "                metrics = calculate_metrics(example['question'], result, example['answer'])\n",
    "                all_metrics.append(metrics)\n",
    "                \n",
    "                # Log metrics for each example\n",
    "                for metric_name, value in metrics.items():\n",
    "                    mlflow.log_metric(f\"example_{i}_{metric_name}\", value)\n",
    "                \n",
    "                # Log summaries as artifacts\n",
    "                example_dir = f\"example_{i}\"\n",
    "                os.makedirs(example_dir, exist_ok=True)\n",
    "                \n",
    "                with open(f\"{example_dir}/predicted_answer.txt\", \"w\") as f:\n",
    "                    f.write(result)\n",
    "                with open(f\"{example_dir}/reference_answer.txt\", \"w\") as f:\n",
    "                    f.write(example['answer'])\n",
    "                \n",
    "                mlflow.log_artifacts(example_dir)\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"Error processing example {i}: {str(e)}\")\n",
    "                continue\n",
    "        \n",
    "        if not all_metrics:\n",
    "            print(\"No successful evaluations completed\")\n",
    "            return None, None\n",
    "            \n",
    "        # Calculate and log average metrics\n",
    "        avg_metrics = {}\n",
    "        for metric in all_metrics[0].keys():\n",
    "            avg_value = np.mean([m[metric] for m in all_metrics])\n",
    "            avg_metrics[f\"avg_{metric}\"] = avg_value\n",
    "            mlflow.log_metric(f\"avg_{metric}\", avg_value)\n",
    "        \n",
    "        # Create and log visualizations\n",
    "        create_and_log_visualizations(all_metrics)\n",
    "        \n",
    "        return run.info.run_id, avg_metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3553bb6a-f85f-4150-8d0e-20df008065eb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Defining metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c1f14ebf-6c40-4f1c-90a8-cac8d4001947",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import re\n",
    "@mlflow.trace(name=\"calculating metrics\")\n",
    "def calculate_metrics(question: str, answer: str, reference_answer: str) -> Dict[str, float]:\n",
    "    \"\"\"Calculate various evaluation metrics\"\"\"\n",
    "    # ROUGE scores\n",
    "    scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\n",
    "    rouge_scores = scorer.score(reference_answer, answer)\n",
    "    \n",
    "    # BLEU score\n",
    "    reference = [reference_answer.split()]\n",
    "    candidate = answer.split()\n",
    "    bleu = sentence_bleu(reference, candidate)\n",
    "    \n",
    "    # Summary length metrics\n",
    "    pred_length = len(answer.split())\n",
    "    ref_length = len(reference_answer.split())\n",
    "    length_ratio = pred_length / ref_length if ref_length > 0 else 0\n",
    "    \n",
    "    return {\n",
    "        'rouge1_precision': rouge_scores['rouge1'].precision,\n",
    "        'rouge1_recall': rouge_scores['rouge1'].recall,\n",
    "        'rouge1_f1': rouge_scores['rouge1'].fmeasure,\n",
    "        'rouge2_f1': rouge_scores['rouge2'].fmeasure,\n",
    "        'rougeL_f1': rouge_scores['rougeL'].fmeasure,\n",
    "        'bleu_score': bleu,\n",
    "        'predicted/reference_length_ratio': length_ratio,\n",
    "        'predicted_length': pred_length,\n",
    "        'reference_length': ref_length,\n",
    "    }\n",
    "\n",
    "def create_and_log_visualizations(metrics_list: List[Dict[str, float]]):\n",
    "    \"\"\"Create and log visualizations to MLflow\"\"\"\n",
    "    # Convert metrics to DataFrame\n",
    "    df = pd.DataFrame(metrics_list)\n",
    "    \n",
    "    # ROUGE scores comparison\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    rouge_metrics = ['rouge1_f1', 'rouge2_f1', 'rougeL_f1']\n",
    "    df[rouge_metrics].mean().plot(kind='bar')\n",
    "    plt.title('Average ROUGE Scores')\n",
    "    plt.ylabel('Score')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('rouge_scores.png')\n",
    "    mlflow.log_artifact('rouge_scores.png')\n",
    "    plt.close()\n",
    "    \n",
    "    # Summary length analysis\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.scatter(df['reference_length'], df['predicted_length'])\n",
    "    plt.plot([0, max(df['reference_length'])], [0, max(df['reference_length'])], '--', color='red')\n",
    "    plt.xlabel('Reference response Length')\n",
    "    plt.ylabel('Predicted response Length')\n",
    "    plt.title('Response Length Comparison')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('length_comparison.png')\n",
    "    mlflow.log_artifact('length_comparison.png')\n",
    "    plt.close()\n",
    "    \n",
    "    # Metrics distribution\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    metrics_to_plot = ['rouge1_f1', 'rouge2_f1', 'rougeL_f1', 'bleu_score']\n",
    "    df[metrics_to_plot].boxplot()\n",
    "    plt.title('Distribution of Evaluation Metrics')\n",
    "    plt.ylabel('Score')\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('metrics_distribution.png')\n",
    "    mlflow.log_artifact('metrics_distribution.png')\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1b93c19b-f1bc-4421-9c90-8f267c68b737",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Prepare dataset for evalaution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2f3e3bf2-f15f-4a53-bfac-ddfff36d282b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def prepare_test_data() -> List[Dict[str, str]]:\n",
    "    \"\"\"Prepare test data with YouTube videos and reference summaries\"\"\"\n",
    "    # Replace with your actual test data\n",
    "    return [\n",
    "        {\n",
    "            \"question\": \"que es epm en colombia\",\n",
    "            \"answer\": \"Es una empresa colombiana de servicios públicos domiciliarios fundada el 6 de agosto de 1955 en Medellín. EPM ofrece servicios de energía eléctrica, gas natural, agua potable, saneamiento básico y telecomunicaciones. Es propiedad del municipio de Medellín y tiene presencia en varias regiones de Colombia y otros países de América Latina.\"\n",
    "        },\n",
    "        {\n",
    "            \"question\": \"why databricks can be written as dbx\",\n",
    "            \"answer\": \"\"\"Databricks can be abbreviated as DBX because it is a shorthand or mnemonic derived from the company’s name, following a common convention in technology and branding:\n",
    "\t1.\t“DB” for Databricks: The “DB” part directly represents “Data” and “Bricks,” reflecting the name of the platform.\n",
    "\t2.\t“X” for Flexibility or Multiplicity:\n",
    "\t•\tThe “X” is often used in technology branding to imply scalability, versatility, or cutting-edge innovation.\n",
    "\t•\tIn some cases, “X” also represents a short, dynamic character to make abbreviations look modern and tech-savvy.\n",
    "\t3.\tCompact Branding:\n",
    "\t•\tUsing DBX is easier to write, remember, and type than the full name “Databricks.”\n",
    "\t•\tMany organizations and products adopt similar shortened names (e.g., GCP for Google Cloud Platform or AWS for Amazon Web Services).\n",
    "\n",
    "Thus, DBX is a convenient, modern shorthand for Databricks, widely used in casual and technical communication.\"\"\"\n",
    "        },\n",
    "        {\n",
    "            \"question\": \"what is an LLM Agent\",\n",
    "            \"answer\": \"An LLM Agent is a software or application framework that uses a Large Language Model (LLM) as a core component to perform complex tasks by reasoning, generating content, or interacting with users or systems. LLM Agents typically extend the capabilities of an LLM to incorporate structured decision-making, external tool usage, and contextual adaptability.\"\n",
    "        }\n",
    "    ]\n",
    "\n",
    "test_data = prepare_test_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d1cb5fea-7310-43c6-9269-485a9db81163",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Experiments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "56cff8e8-963c-454e-8880-de2389957091",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Experiment 1: Basic prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e672076b-5aee-4ab8-881e-77e86f69df11",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "model_name = \"databricks-meta-llama-3-1-70b-instruct\"\n",
    "\n",
    "chain = myChain(model_name=model_name)\n",
    "\n",
    "run_id, model_uri = log_chain_to_mlflow(chain=chain, \n",
    "                             wrapper=myChainWrapper, experiment_name=\"/Users/guillermo.angarita.gutierrez@gmail.com/example-chain\")\n",
    "\n",
    "print(f\"Chain logged with run_id: {run_id}\")\n",
    "\n",
    "# Load the chain\n",
    "ml_flow_chain = mlflow.pyfunc.load_model(model_uri)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "35ac8ce0-ba10-4115-bf3c-358a884fb8a0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Prepare test data\n",
    "test_data = prepare_test_data()\n",
    "experiment_name = \"/Users/guillermo.angarita.gutierrez@gmail.com/example-chain\"\n",
    "# Run evaluation\n",
    "run_id, avg_metrics = evaluate_model_with_mlflow(ml_flow_chain, test_data, experiment_name)\n",
    "\n",
    "print(\"\\nEvaluation Results:\")\n",
    "print(\"==================\")\n",
    "for metric, value in avg_metrics.items():\n",
    "    print(f\"{metric}: {value:.4f}\")\n",
    "\n",
    "print(f\"\\nMLflow run ID: {run_id}\")\n",
    "print(\"View detailed results in the MLflow UI\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9821a086-3886-40dd-ac06-28fba50c9d27",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "avg_metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "77263cf3-8311-4c44-8282-e39f51816f7e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Register best model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3e0cfa0b-aba0-433d-ad8c-a383af261e71",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Log the final model to Databricks Model Registry\n",
    "model_name = \"llm_qa_chain\"\n",
    "model_version = mlflow.register_model(\n",
    "    model_uri=model_uri,\n",
    "    name=model_name  # Remove the \"models:/\" prefix\n",
    ")\n",
    "\n",
    "# Add description and tags\n",
    "client = mlflow.tracking.MlflowClient()\n",
    "client.update_registered_model(\n",
    "    name=model_name,  # Use simple model name\n",
    "    description=\"Question-Answering Chain using Llama 3 70B\"\n",
    ")\n",
    "\n",
    "# Add version-specific tags\n",
    "client.update_model_version(\n",
    "    name=model_name,  # Use simple model name\n",
    "    version=model_version.version,\n",
    "    description=\"Basic QA chain with simple prompt template\"\n",
    ")\n",
    "\n",
    "print(f\"Model registered as: {model_name}\")\n",
    "print(f\"Model version: {model_version.version}\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "client": "1"
   },
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "agent",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
